{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n - Day4 (Lec 8-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning frameworks\n",
    "\n",
    "We have Caffe, PyTorch, Tensorflow, and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN architectures - ILSVRC winners\n",
    "\n",
    "#### 2012 - AlexNet (8Layers - 16.4%)    \n",
    "![AlexNet](images/Alexnet.png)\n",
    "[Source](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "* First adaptation of CNN architecture.\n",
    "* Due to lack of GPU memory, the architecture had to be separated into 2 segments in which the layers from different segments didn't interact with each other except only once.\n",
    "* Details\n",
    "    - first use of ReLU\n",
    "    - used Norm layers (not common anymore)\n",
    "    - heavy data augmentation\n",
    "    - dropout 0.5\n",
    "    - batch size 128\n",
    "    - SGD Momentum 0.9\n",
    "    - Learning rate 1e-2, reduced by 10 manually when val accuracy plateaus\n",
    "    - L2 weight decay 5e-4\n",
    "    - 7 CNN ensemble: 18.2% -> 15.4%\n",
    "\n",
    "\n",
    "#### 2013 - ZFNet (8Layers - 11.7%)\n",
    "* A version of AlexNet with better hyperparameters.\n",
    "\n",
    "#### 2014 - VGG Net (19Layers - 7.3%)\n",
    "![VGG16](images/VGG16.png)\n",
    "[Source](https://www.cs.toronto.edu/~frossard/post/vgg16/)\n",
    "* Using smaller filters was a better idea. 3 stacks of 3x3 conv layers had the same _effective receptive field_ of one 7x7 conv layer, except that the former had less parameters, more depth, and more non-linearities.\n",
    "* This allowed them to introduce deeper networks, and thus better results.\n",
    "* It is said that the features of \"FC7\" layer in the architecture generalize well to other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2015 - GoogLeNet (22Layers - 6.7%)\n",
    "* At this point a huge problem was the number of parameters. Just 3 FC layers with 4096 inputs and outputs require 50 million parameters.\n",
    "* GoogLeNet on the other hand used only 1(2+2 on auxillary classifiers) FC layer. They implemented an *\"Inception\"* module, with which they could achieve 12x less parameters than AlexNet. Each parts perform different types of operations in parallel, of which stride/padding is set in a way such that their results can be concatenated depthwise.\n",
    "![Bad Inception](images/inceptionNaive.png)\n",
    "* It's a good thing that we have significantly less parameters. However, the problem here is that this still takes _sooo_ much to compute. And because we are concatenating 4 outputs into one, the output will be _soooooo_ much deeper than the input.\n",
    "* To solve this, GoogLeNet implemented a 1x1 conv _\"bottleneck\"_ layers. These 1x1 conv layers preserve spatial dimention(width/height), but reduce their depth. \n",
    "![Better_Inception](images/inceptionEarnest.png)\n",
    "[Source](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)\n",
    "* So before 3x3 and 5x5 conv, we can reduce the depth of input matrix and save a ton of computation! We can also reduce the depth of the output with bottleneck after pooling layer.\n",
    "![GoogLeNet](images/GoogLeNet.png)\n",
    "* One last problem was that the architecture itself could not pass the gradient deep enough into the network. So the network had 2 more auxillary classifiers, which they also produced loss and provided gradients to earlier layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016 - ResNet (152Layers - 3.57%)\n",
    "* The first network to surpass **human accuracy**.\n",
    "* What the engineers of ResNet found out was that the problem of deep networks wasn't the overfitting, but that they *cannot* overfit at the first place. The deeper vanilla CNN was, their training error become worse.\n",
    "* The biggest obstacle is that the gradient cannot flow deep enough (just like in GoogLeNet, where they had to put 2 extra gradient sources).\n",
    "* They solved this by introducing *\"Residual blocks\"*, where they don't have to compute the whole transformation. Instead, they choose *how much change* they will give to the input.\n",
    "![Residual_block](images/Residual.png)\n",
    "[Source](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec)\n",
    "* Where the input $x$ is passed directly to the end of the block. In original CNN, the network would have trained $x+F(x)$. The residual block only trains towards $F(x)$, but still gives the same result, $x+F(x)$.\n",
    "* Now in backprop, the local gradient is again passed directly to the another end of the block. This means that the earlier layers can learn as fast as the final layers, and thus the whole network can have a better performance.\n",
    "* The bottleneck layer from GoogLeNet was also used for deeper ResNet networks.\n",
    "![Residual_bottleneck](images/Residual_bottleneck.png)\n",
    "[Source](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/residual_net.html)\n",
    "* Details\n",
    "    - Batch Normalization after every CONV layer\n",
    "    - Xavier/2 initialization from He et al.\n",
    "    - SGD + Momentum (0.9)\n",
    "    - Learning rate: 0.1, divided by 10 when validation error plateaus\n",
    "    - Mini-batch size 256\n",
    "    - Weight decay of 1e-5\n",
    "    - No dropout used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other CNN architectures\n",
    "#### 1. Network in Network (NiN)\n",
    "* \"Micronetwork\" within each conv layers that allows to compute more abstract features of local pixel groups.\n",
    "* Its usage of multilayer perceptron was a precursor to bottleneck layers.\n",
    "* Philosophical inspiration for GoogLeNet.\n",
    "\n",
    "#### 2. Improving ResNet\n",
    "* ResNet engineers improved the residual block, figuring out that batch-normalizing before every ReLU gives the best performance.\n",
    "![residual](images/better_residual.png)\n",
    "[source](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec)\n",
    "\n",
    "#### 3. Wide ResNet\n",
    "* Arguing that the residual itself is more important than depth, they also put more filters in each layer.\n",
    "* A wide 50-layer was better than original 152-layer ResNet.\n",
    "* It's also computationally efficient since they can easily be parallelized.\n",
    "\n",
    "#### 4. ResNeXt\n",
    "* Instead of a wide block, have multiple thinner pathways that concatenate afterwards(a bit like inception model).\n",
    "\n",
    "#### 5. Stochastic depth (Dropout Layer)\n",
    "* Randomly dropout several layers (pass identity), and use full deep network at test time.\n",
    "\n",
    "#### 6. FractalNet\n",
    "* Arguing that the ability to transition between different depths is important, used a fractal-like architecture that allows multiple depths to choose from.\n",
    "* Trained with dropping out sub-paths, use full paths at test time.\n",
    "\n",
    "#### 7. DenseNet\n",
    "* Dense blocks are connected to every other layers after them.\n",
    "\n",
    "#### 8. SqueezeNet\n",
    "* AlexNet accuracy with 50x less parameters.\n",
    "* Modules consist of 'squeeze' layer of 1x1 filters that connects to 'expand' layer with 1x1 and 3x3 filters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
