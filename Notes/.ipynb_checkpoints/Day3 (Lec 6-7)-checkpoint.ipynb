{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n - Day3 (Lecture 6-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "#### 2 layer neural network\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x & \\rightarrow & Wx + b & \\rightarrow & s & \\Rightarrow & Prediction \\\\\n",
    "3072 \\times 1 & & (10\\times 3072)(10\\times 1) & & 10 \\times 1\n",
    "\\end{matrix}\n",
    "$$ <br>\n",
    "$$\n",
    "\\downarrow\n",
    "$$ <br>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "x & \\rightarrow & W_1x + b & \\rightarrow & max(W_1x+b) & \\rightarrow & W_2(max(W_1x+b)) & \\rightarrow& s & \\Rightarrow & Prediction \\\\\n",
    "3072 \\times 1 & & (100\\times 3072)(100\\times 1) & & 100 \\times 1 & & (10 \\times 100) (10 \\times 1) & & 10\\times 1\n",
    "\\end{matrix}\n",
    "$$ <br>\n",
    "   * The important layer here is the $max$ function, because of its non-linearity. WIthout a non-liear property between the two linear functions ($W_1, W_2$), a combination of linear functions will be another linear function. As they say, \"the non-linearity is where we get the *wiggle*\"\n",
    "    \n",
    "#### Biological inspiration\n",
    "* A neuron model\n",
    "    * Neurons have multiple dendrites, a single axon, and multiple axon terminals. Inputs from other neurons or sensors are connected to *dendrites* via *synapses*.\n",
    "    * Each synapses have its *synaptic strength*($w_0$), which interacts multiplicatively to the input signal($x_0 \\rightarrow w_0x_0$). With these coefficients, neurons can amplify or neglect certain input signals.\n",
    "    * In a simple neuron, these signals travel through the dendrite and gather in the *cell body*, where they all get summed.\n",
    "    * If the total sum is over a certain threshold, the neuron *fires* a signal, with its frequency proportional to the strength of signal. The frequency is determined by an *activation function*.\n",
    "    * So basically\n",
    "    $$\n",
    "    \\begin{matrix}\n",
    "    \\rightarrow & x_0 & \\rightarrow & w_0x_0 &             &               &             & \\\\\n",
    "                &     &             &        & \\searrow    &               &             & \\\\\n",
    "    \\rightarrow & x_1 & \\rightarrow & w_1x_1 & \\rightarrow & \\sum_i w_ix_i & \\rightarrow & f(\\sum_i w_ix_i) \\rightarrow\\\\\n",
    "                &     &             &        & \\nearrow    &               &             & \\\\\n",
    "    \\rightarrow & x_2 & \\rightarrow & w_2x_2 &             &               &             &\n",
    "    \\end{matrix}\n",
    "    $$\n",
    "    \n",
    "    \n",
    "* **Problem** : It's extremely coarse.\n",
    "    * The synapses are not linear weights, but a complex non-linear dynamic system.\n",
    "    * The model omits the timing of output spike, which is known to be important.\n",
    "    * [pdf1](https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf) [pdf2](https://www.sciencedirect.com/science/article/abs/pii/S0959438814000130)\n",
    "    \n",
    "    \n",
    "* Interpretation as a linear classifier\n",
    "    * As you probably already noticed, the neuron model is quite similar to a linear classifier. Well actually, it is! The only difference is that neuron model doesn't yet have a loss function.\n",
    "    * Training with sigmoid activation and cross-entropy loss would lead to a binary softmax classifier(a.k.a logistic regression). Since the result of sigmoid function is between (0,1), the score can be a possiblity($p$) of one of the two classes, and the other being the remainder($1-p$).\n",
    "    * In the same sense, using max activation and hinge loss will give a binary SVM classifier.\n",
    "    * Regularization can be considered as *gradual forgetting*, from the effect of regularization of forcing all weights($w$) to be simpler(near 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of sigmoid\n",
    "x = np.arange(-10, 10, .1)\n",
    "sig = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot([-10, 10],[0, 0], c='k')\n",
    "plt.plot([0, 0],[0, 1], c='k')\n",
    "plt.plot(x, sig, c='orange')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "#### 1. Sigmoid\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "* Originally frequently used, due to its \"seemingly\" nice appeal to biological interpretation (Threshold and firing whatnot).\n",
    "* Not used nowadays. Apparently it was a terrible idea.\n",
    "* 2 major problems\n",
    "    * Dying gradients  \n",
    "    When the input values to the sigmoid layer have large absolute values, their local gradients will be near 0. This means that this gradient cannot make meaningful updates to the connected weights, thus the 'kill' of gradients.  \n",
    "    Also if weight initialization is poorly done, the starting point could be these 'saturated' points, leading to slow or no learning.\n",
    "       <br> <br>\n",
    "    * Not zero-centered  \n",
    "    In an extreme case like sigmoid, a layer's output is purely positive. This is a problem for the neurons **after** this layer. The neuron right after the sigmoid will only get positive inputs. The local gradient is related to the gradient from future layer and the inputs. And since all inputs were positive, the local gradients will either all be positive or negative, depending on the gradient from future layer.  \n",
    "    This is a huge deal, since we'll have extremely limited directions. For n-variables, we only have $\\frac{1}{2^n}$ region of choice from the original. We cannot make optimal steps with this constraint.\n",
    "    <br> <br>\n",
    "    * Computationally expensive  \n",
    "    Because exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of tanh\n",
    "x = np.arange(-10, 10, .1)\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "plt.plot([-10, 10],[0, 0], c='k')\n",
    "plt.plot([0, 0],[-1, 1], c='k')\n",
    "plt.plot(x, tanh, c='orange')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.title(\"Tanh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tanh\n",
    "$$\n",
    "f(x) = tanh(x) = 2\\sigma (2x) - 1\n",
    "$$\n",
    "* Solved zero-centering, still saturates.\n",
    "* Basically a scaled version of sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of ReLU\n",
    "x = np.arange(-10, 10, .1)\n",
    "ReLU = np.maximum(x, np.zeros(len(x)))\n",
    "\n",
    "plt.plot([-10, 10],[0, 0], c='k')\n",
    "plt.plot([0, 0],[0, 10], c='k')\n",
    "plt.plot(x, ReLU, c='orange')\n",
    "plt.title(\"ReLU\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ReLU (Rectified Linear Unit)\n",
    "$$\n",
    "f(x) = max(0,x)\n",
    "$$\n",
    "* The most common one in practice\n",
    "    * Converges [6x](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) faster than sigmoid/tanh. It is said that this is due to its linear, non-saturating form.\n",
    "\n",
    "    * No saturation in positive region\n",
    "\n",
    "    * Computationally much more efficient (only max function)\n",
    "\n",
    "    * Actually more biologically plausible!\n",
    "\n",
    "\n",
    "* But still some limits...\n",
    "    * Saturates in negative region  \n",
    "    This means if the local classifier(linear part of a neuron) is *'pushed'* out of data cloud, the ReLU after that will only output 0. And most importantly the local gradient will all be 0, thus kiling ReLU and the linear part.\n",
    "    This also means **ReLU is sensitive to large learning rates**. ReLU easily dies when a large gradient flows through. As much as 40% of all ReLU can be dead if the learning rate is too high.  \n",
    "    It may be practical to **initialize ReLU neurons with small positive biases**. (e.g. 0.01)\n",
    "    <br> <br>\n",
    "    * Not zero centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5hU5dnH8e/N0osCsvRqwxZFsioWwGCJUaxRX2PsJmp8jfIaTTSmaExRY4wmVmKJUWMJohJi7NJE0BVBQVRAkSqgCEhnd+/3j3PQYZmF2WXnPFN+n+vaa2ZOmfM7Z87ec+Y5Z54xd0dERIpHg9ABREQkWSr8IiJFRoVfRKTIqPCLiBQZFX4RkSKjwi8iUmRU+CUvmNk5ZjYudA6RQqDCL/XOzGab2eGhc6QTv4FUmtlKM1thZlPMbHAt5h9lZj9IM/xQM5uX6fTVpvnQzHatYd61cdbPzGy4mXWKx/3dzNbH45aa2Ytmtlst1sPNbOc0w681s4cznV7ykwq/FKPX3b0l0Bq4E3jMzFqHCGJmOwEN3P3DGia5JM66K1HeP6eMuyke1wWYD9yX1bBSMFT4JVFmNtjMJpvZMjMbb2Z7p4y7ysxmmdmXZvaemZ24hef5o5mNM7Pt4yPeb6SMa29ma8ysdEtZ3L0KeAhoAeySMn+/ONuy+BPBoduyzltxDPDs1iZy96XAk8BeacatAZ4A+qQON7PzzGy6mX1hZs+bWY96yix5ToVfEmNmfYH7gQuBHYB7gBFm1iSeZBbQH9geuA54eGPTRspzNDCzvwF7A0e6+3LgMeCMlMm+B7zk7ku2kqcEOBfYAHwSD+sC/Af4LdAWuAJ4cmtvItvg6Hh5W2Rm7YDvAm+nGdeCaJ1npgw7Afg5cBJQCowFHq2fyJLvVPglST8E7nH3ie5e6e4PAuuAfgDu/i93X+DuVe7+ODAD2D9l/kZExastcKy7r46HPwicbmYb9+cziY7ka9LPzJYBa4GbgTPcfXE87gzgWXd/Ns7xIlBOVKDrlZk1B/YDRm9hsr/EWacAC4HLU8ZdEY/7EjiEaL03uhD4g7tPd/cK4PdAHx31C6jwS7J6AD+Jm1CWxUWrG9AZwMzOSmkGWkbUrNEuZf6dgeOB69x9/caB7j4RWAUMjE9w7gyM2EKOCe7eGmgTT9e/WsZTqmU8BOiU5nlSVRC9MVXXiOgTRTqHAePdfe0WnvdSd2/t7l3c/fvVPsXcHK9HT2AN0LvaetyWsg5LASM6H1Cr9TCzjY9rWg/JMyr8kqS5wO/iQrbxr7m7Pxofif4NuATYIS5oU4mK1UbTiZpm/mtmvas994NER+tnAsO2UkwBcPeVwMXAmWa2b0rGh6plbOHuN2zl6eYA7cys5cYBZmZEBfiTGubJqJlna9x9DnAZUaFvFg+eC1xYbT2aufv4rTzdHKI3klS9gEqiE8hSAFT4JVsamVnTlL+GRIX9IjM7wCItzOwYM2tFdILVgSUAZnYu6U9kPkrUdv1SfEXMRg8BJxIV/39kGtLdPwfuBX4VD3oYONbMvm1mJXH2Q82sa8psDautW6O4+E4EbjSzlvF5iyuJjqAn1LD475DBid0M1+NFYAFwQTzobuBqM9sTID4Jfkq12RpXW48S4Dmgt5mdaWaNzKwtUTPRsLjJSAqACr9ky7NEzQ8b/65193Kidv7bgS+ITkaeA+Du7wF/Al4HFgHfAF5L98TxuYHfAK+YWc942DxgEtGbx9haZr0VONrM9nb3uUTNST8nehOaS1TAU/9X7qq2bg/Ew/8HaB+v13yippyj0336MLO9gJXxG0Z9+SPwUzNr4u5PATcSXaq6gujT03eqTT+t2nqcG5/rOJroHMHieL7lwI/qMacEZvohFikUZnY/sMDdfxE6y9aY2U+Bdu7+09BZpPg0DB1ApD7ER/4nAftuecqcMRv4d+gQUpzU1CN5z8yuJ2qS+KO7fxw6Tybc/Ql3nx46hxQnNfWIiBQZHfGLiBSZvGjjb9eunffs2TN0DBGRvPLWW2995u6bdTeSF4W/Z8+elJeXh44hIpJXzCztlwfV1CMiUmRU+EVEiowKv4hIkVHhFxEpMir8IiJFJmuF38zuN7PFZjY1ZVjb+EehZ8S3bbK1fBERSS+bR/x/B46qNuwq4GV33wV4OX4sIiIJylrhd/cxRL/6k+p4oh/MIL49IVvLF8m2IUOGMGTIkNAxRGot6S9wdXD3hQDuvtDM2tc0oZldQPyjEt27d08onkjmJk+eHDqCFCp3MNv6dHWUsyd33X2ou5e5e1lp6WbfOBYRKUzuMOEc+OD2rC0i6cK/yMw6AcS3ixNevohIbpv6W/j4H1CxImuLSLrwjwDOju+fDTyT8PJFRHLXJ4/Du7+CXmfBHldnbTHZvJzzUaLfT+1tZvPM7HzgBuAIM5sBHBE/FhGRzybA62dDaX/Yf2hW2/izdnLX3b9Xw6jDsrVMEZG8tHI2jDkemneF/sOhpElWF5ezJ3dFRIrChhUw+lioXA8DR0LTdllfZF70xy8iUpCqKmDcabBiOnzredh+t0QWq8IvIhLKpMth4X9h/3ugY3Kt4GrqEREJ4cM74MO/wm6Xw84XJLpoFX4RkaQteA7euhS6HAd9bkp88Sr8IiJJWjYVxp0KrfeGgx6BBiWJR1DhFxFJytrFMHowNGoJA/8d3Qagk7siIkmoXAtjToiK/+Fjomv2A1HhFxHJNneYcB589jr0fxJ2KAsaR009IiLZ9u518MmjsM8foNtJodOo8IuIZNXsf8LU62DHc2GPn4VOA6jwi4hkz5LxMOFcaD8A9rs7qx2v1YYKv4hINqz8ODqZ27x73PFa49CJvqLCLyJS39Yvjy7brNoAh46EJjuETrQJXdUjIlKfqiqiL2it+BAGvQDb9Q6daDMq/CIi9cUd3roMPn0BDrgXOnwrdKK01NQjIlJfPrwdZtwJu18JO50fOk2NVPhFROrD/Gdh0hDoenx0vX4OU+EXEdlWy96F106D1vsE63itNlT4RUS2xZpPYdRgaNQq6nitYYvQibZKJ3dFROqqYk10rf66z+CIsdC8S+hEGVHhFxGpC6+CCefA529EX9Bq2zd0ooyp8IuI1MW718KcJ6DPjdDthNBpakVt/CIitfXxwzD1etjxvOjSzTyjwi8iUhuLx8HE86MvZ+13V850vFYbKvwiIpla+RGMPRFa9IRDhuVUx2u1ocIvIpKJ9cuiyza9EgaOhCZtQyeqsyCF38z+z8ymmdlUM3vUzJqGyCEikpGqDVHHaytnRlfwbLdL6ETbJPHCb2ZdgEuBMnffCygBTks6h4hIRtyh/FL49EXY7x7ocGjoRNssVFNPQ6CZmTUEmgMLAuUQEdmyD26DmXdHP5u407mh09SLxAu/u88HbgbmAAuB5e7+QvXpzOwCMys3s/IlS5YkHVNEBOaPhEmXRz+Qvs/vQ6epNyGaetoAxwO9gM5ACzM7o/p07j7U3cvcvay0tDTpmCJS7L6YEnW81rYvHPgPsMK5FibEmhwOfOzuS9x9AzAcOChADhGR9NYshNHHQqPWMGBEXnS8VhshCv8coJ+ZNTczAw4DpgfIISKyuYrVMPp4WPd51Ntm886hE9W7EG38E4FhwCTg3TjD0KRziIhsxqvg9bNhaTkc/Ci03Td0oqwI0kmbu/8a+HWIZYuI1OidX8LcYbDvzdD1uNBpsqZwzlaIiGyLj/4B034PO/0Qdrs8dJqsUuEXEVk8Ft74AXQYBPvdkZcdr9WGCr+IFLcvZ0Ydr7XcEfoPgwaNQifKOhV+ESle67+A0YOjbhkGjoTGbUInSoR+gUtEilPVBhh7ctTV8qCXoNXOoRMlRoVfRIqPO5RfAotegX5/h/YDQidKlJp6RKT4vP9nmDkU9rgadjw7dJrEqfCLSHGZNwLevgK6nQz7/DZ0miBU+EWkeHwxGcafDm3L4MAHC6rjtdoozrUWkeKzekH004mN28DAZ6Bh89CJgtHJXREpfBWrYcxxsGEZHPEaNOsUOlFQKvwiUti8Cl4/E5ZOggHPQJt9QicKToVfRArblGtg7nDoewt0PTZ0mpygNn4RKVyzHoD3boCdL4TeQ0KnyRkq/CJSmBaNhjcvhI6HQ9lfC77jtdpQ4ReRwrNiBow9CVruBIf8qyg6XqsNFX4RKSzrlkYdr5nFHa+1Dp0o5+jkrogUjsr1MO5kWDUbBr0MrXYKnSgnqfCLSGFwh/KLYdGrcOBD0P6Q0Ilylpp6RKQwvP8nmHUf7PkL6HVG6DQ5TYVfRPLf3Kfh7Z9C91Ng7+tCp8l5Kvwikt+WToLx34cd9oN+xdvxWm1oC4lI/lo9H0YfC03aRd0xNGwWOlFe0MldEclPFauior9hRdzxWsfQifKGCr+I5B+vgvFnwLIpMGAEtNk7dKK8osIvIvln8tUw72noeyt0OSZ0mryjNn4RyS+z7oPpN8EuP4Lel4ZOk5eCFH4za21mw8zsfTObbmYHhsghInlm0avwxkXQ8Uj45l/U8VodhWrquQ14zt1PNrPGQPH+BpqIZGbFhzD2u7DdrnDIE9BALdV1lfiWM7PtgAHAOQDuvh5Yn3QOEckj6z6HUceAlcQdr20fOlFeC9HUsyOwBHjAzN42s3vNrEX1iczsAjMrN7PyJUuWJJ9SRHJD5froSH/1HBjwNLTsFTpR3gtR+BsCfYG73H1fYBVwVfWJ3H2ou5e5e1lpaWnSGUUkF7jDmxfB4tFwwP1QenDoRAUhROGfB8xz94nx42FEbwQiIpuafhN89ADs9Svo9f3QaQpG4oXf3T8F5ppZ73jQYcB7SecQkRw3dzhMvgp6nAbfuDZ0moIS6rT4j4FH4it6PgLODZRDRHLR0reib+bu0A/6PaDLNutZkMLv7pOBshDLFpEct3pe1AdP0/bRydySpqETFRxdCCsiuWPDyrjjtZVw5Hho1iF0ooKkwi8iuaGqMupXf9k7MPA/0Hqv0IkKlgq/iOSGyT+D+SPgm3+FzkeFTlPQ1EmbiIQ382/Rb+buegn0viR0moKnwi8iYX36Mrx5MXQ6Cvr+OXSaoqDCLyLhLH8fxp4M2/WGgx9Tx2sJUeEXkTDWfgajB0NJY3W8ljC9vYpI8irXwdiTomv2D3sVWvYMnaioqPCLSLLc4Y0LYclYOOhRKNXvMCVNTT0ikqz3boCPH4z63+l5Wug0RUmFX0SSM2cYTPk59Dg96nFTglDhF5FkfP4mvH4mtDsI+t2njtcCUuEXkexbNQdGHwdNO8KAp9TxWmA6uSsi2bXhy6jjtcrVMOilqNdNCUqFX0Syp6oSXjsdlk+LO17bM3QiQYVfRLLp7SthwUgouwM6fzt0GompjV9EsmPGPfDBn2HXS2HXi0OnkRQq/CJS/xa+COX/C52Phr63hE4j1ajwi0j9Wj4dxp0C2+0OBz8KDUpCJ5JqVPhFpP581fFaUzh0JDTaLnQiSUMnd0WkflSug7EnwpoFcNgoaNEjdCKpQUZH/GZ2ipm1iu//wsyGm1nf7EYTkbzhDhN/AEvGQb8Hod0BoRPJFmTa1PNLd//SzA4Bvg08CNyVvVgiklem/Q5mPwx7Xw89Tg2dRrYi08JfGd8eA9zl7s8AjbMTSUTyyidPwDu/hJ5nwJ7XhE4jGci08M83s3uAU4FnzaxJLeYVkUL12USYcDaUHgwH3KuO1/JEpsX7VOB54Ch3Xwa0Ba7MWioRyX2rPoExx0OzztD/KShpEjqRZCijq3rcfTUwPOXxQmBhtkKJSI7bsCLueG1t9NOJTUtDJ5JaCNZcY2YlZva2mY0MlUFE6qCqAl77Hix/Dw75F2y/e+hEUksh2+kvA6YHXL6I1MXbV8CCZ6Hsduh0ROg0UgdBCr+ZdSW6QujeEMsXkTqacRd8cBv0HgK7XBQ6jdRRqCP+W4GfAlU1TWBmF5hZuZmVL1myJLlkIpLegueh/MfQeTDse3PoNLINEi/8ZjYYWOzub21pOncf6u5l7l5WWqoTRyJBLZsGr50K2+8JB/9THa/luRBH/AcDx5nZbOAxYJCZPRwgh4hkYu3iuOO15jBwJDRqFTqRbKPEC7+7X+3uXd29J3Aa8Iq7n5F0DhHJQOVaGHMirP0UBjwDLbqFTiT1QL1zikh67jDhfPhsfHTZZrv9QyeSehK08Lv7KGBUyAwiUoOp18Mn/4R9fgfdTw6dRuqR+tsRkc3Nfgze/TX0Ohv2uDp0GqlnKvwisqklr8OEc6C0P+x/jzpeK0Aq/CLytZWzYewJ0Lwr9B+ujtcKlAq/iES+6nhtfXTZZtN2oRNJluiqHhGJOl4b9z+w4n341nOw/W6hE0kWqfCLCEz6P1j4HOw/FDoeFjqNZJmaekSK3Qe3w4e3w24/gZ1/GDqNJECFX6SYLXgOJl0GXY6DPjeGTiMJUeEXKVbLpsK4U6H13nDQI+p4rYio8IsUozWLoo7XGrWEgf+ObqVo6OSuSLGpWANjToh63TxibHTNvhQVFX6RYuIOE8+DzydA/yeh7TdDJ5IA1NQjUkzevQ4+eQz2+QN0Oyl0GglEhV+kWMz+J0y9DnY8F/b4Weg0EpAKv0gxWDIeJpwL7QfCfner47Uip8IvUuhWfhydzG3RI2rXL2kcOpEEpsIvUsjWL48u2/SKqOO1JjuETiQ5QFf1iBSqqoroC1orPoRBL8B2u4ZOJDlChV+kELnDW5fBpy/AAfdCh2+FTiQ5RE09IoXow7/CjDth9ythp/NDp5Eco8IvUmjm/yfqZrnrCdDnhtBpJAep8IsUkmXvwmunQet94KCHwfQvLpvTXiFSKNZ8CqMGQ6Ptoo7XGrYInUhylE7uihSCjR2vrfss7nitS+hEksNU+EXynVfBhHPg8zeg/3Bo2zd0IslxKvwi+e6dX8OcJ6DPTdDthNBpJA+ojV8kn338MEz7bXTJ5u5XhE4jeSLxwm9m3czsVTObbmbTzOyypDOIFITF42Di+dGXs8ruVMdrkrEQTT0VwE/cfZKZtQLeMrMX3f29AFlE8tPKj2DsidCiJxwyTB2vSa0kfsTv7gvdfVJ8/0tgOqBLEEQytX4ZjDomOqk7cCQ0aRs6keSZoG38ZtYT2BeYmGbcBWZWbmblS5YsSTqaSG6q2gDjToGVs6IreLbbJXQiyUPBCr+ZtQSeBIa4+4rq4919qLuXuXtZaWlp8gFFco07lP8YPn0J9rsHOgwMnUjyVJDCb2aNiIr+I+4+PEQGkbzzwW0w857oZxN3Ojd0GsljIa7qMeA+YLq735L08kXy0vyRMOny6AfS9/l96DSS50Ic8R8MnAkMMrPJ8d/RAXKI5IcvpkQdr7XtCwc+pI7XZJslfjmnu48DdMGxSCbWLIx+OrFRaxgwAho2D51ICoC6bBDJVRWrYfTxsG4pHDEOmncOnUgKhAq/SC7yKnj9bFhaDgOehrb7hk4kBUSFXyQXvfNLmDsM9r0Zuh4XOo0UGJ0lEsk1Hz0I034PO/0Qdrs8dBopQCr8Irlk8Rh444fQYRDsd4c6XpOsUOEXyRVfzoQxJ0LLHaH/MGjQKHQiKVAq/CK5YP0X0WWbEHW81rhN2DxS0HRyVyS0qg0w9uSoq+VBL0GrnUMnkgKnwi8Skju8+b+w6BXo9yC0HxA6kRQBNfWIhPT+LTDrb7Dnz2HHs0KnkSKhwi8SyrwR8PaV0O1k2Pv60GmkiKjwi4TwxWQYfzq0LYMDH1THa5Io7W0iSVu9AEYNhsZtYeAz6nhNEqeTuyJJqlgFY46DDcujjteadQqdSIqQCr9IUrwKXj8Llk6CgSOgzT6hE0mRUuEXScqUa2DucOh7C3QZHDqNFDG18YskYdYD8N4NsPOF0HtI6DRS5FT4RbJt0Wh480LoeASU/VUdr0lwKvwi2bRiBow9CVruDIc8oY7XJCeo8Itky7qlMPqY6Aj/0JHQuHXoRCKATu6KZEflehh3Mqz6BAa9HHW1LJIjVPhF6ps7lF8Mi16FAx+C9oeETiSyCTX1iNS36TfDrPtgz19ArzNCpxHZjAq/SH2a+zRM/hl0PxX2vi50GpG0VPhF6svSSTD++7DDftDv7+p4TXKW9kyR+rB6Pow+Fpq0gwHPQMNmoROJ1Egnd0W2VcWqqOhvWAFHvAbNOoZOJLJFQY74zewoM/vAzGaa2VUhMojUB8Nh/BmwbAoc/Di02Tt0JJGtSrzwm1kJcAfwHWAP4HtmtkfSOUTqwwUHfATznoa+f4YuR4eOI5KREE09+wMz3f0jADN7DDgeeK++FzRkyBAmT55c308rRaZRgypKW6yjfcu1dGwV3XZouY7fDFzCgN6VPDWtM7cNfRIYHjqqFJg+ffpw66231vvzhij8XYC5KY/nAQdUn8jMLgAuAOjevXsyyaQIOds1qfiqmHeIb9tvvN9qHTs0X7/ZXJ+vbszHi+DOlxvz5Ec7A+p4TfJHiMKf7j/ENxvgPhQYClBWVrbZ+Exk451S8kzlelgzH1bNibpPWD0nur86frxqDlSu3nSekqbQvDu06P71bYseKcO6skNJU7576KEAjBo1KvHVEtkWIQr/PKBbyuOuwIIAOSTfucOGZV8X9a8KekqRX7OQzY4rmraPivj2e0Cno6Kinlrkm5Sq62QpaCEK/5vALmbWC5gPnAacHiCH5LqqDbBmQZqiPgdWx8MqVm46T4PGXxfwTkdC87iobyzszbvpGnspeokXfnevMLNLgOeBEuB+d5+WdA7JAeuXb9rkstnR+oLod2pTNWkXFfBWu0KHwzc/Wm/aXt+YFdmKIF/gcvdngWdDLFsSUlURNbPU1K6+ek70hadUDRpFR+TNu0OHQdXa1ePbhs3DrI9IAdE3d6VuNnxZ85H6qjnRCVWv3HSexm2j4t1yR+jwrc1PnjbtoKN1kQSo8Mvmqiph7cKU9vQ0hX3Dsk3nsYbQvGtUxNsP3LRdfeNto5Zh1kdENqHCX4w2rKzW/FL9Usd54BWbztOo9dcFvLR/mqP1jtCgJMz6iEitqPAXGq+CNZ/W3K6+ag6sX7rpPFYCzbpERbz04Grt6j2gRTdotF2Y9RGReqfCn28qVm/hSH0OrJ4bXQaZqmGrr69+aXfg5oW9WSdooF1BpFjovz2XeBWsXbzlwr7us03nsQbQrHNUxHc4ALqfsvm3TRtvH2Z9RCQnqfAnqWJNdESerqiv2ni0vm7TeRq2iAt4j+iXnTZpgukeFf0GjcKsj4jkJRX++uIO65bU3K6+ek50NL8Ji5pZWvSAtt+EbiduerTeont0UlXdB4hIPVLhz1Tl2uhqly0V9sq1m85T0vzrIt5m3zRH612gpHGY9RGRoqXCD/HR+uc1t6uv+gTWLtp8vqYd46K+D3Q5dvNeHBu31dG6iOSc4ij8lethzbz0vThu7Oyrcs2m85Q0/bqIdxm86ZF686hrXkqahFkfEZFtUNiF/42LYP6I6Lr2zbrm7RB3zbsXdDp603b15t2jzsB0tC4iBaiwC3+LHlF/6+mO1tU1r2yjPn36hI4gUifmXqcft0pUWVmZl5eXh44hIpJXzOwtdy+rPlxdIYqIFBkVfhGRIqPCLyJSZFT4RUSKjAq/iEiRUeEXESkyKvwiIkVGhV9EpMjkxRe4zGwJ8EkdZ28HfLbVqZKnXLWjXLWjXLVTqLl6uHtp9YF5Ufi3hZmVp/vmWmjKVTvKVTvKVTvFlktNPSIiRUaFX0SkyBRD4R8aOkANlKt2lKt2lKt2iipXwbfxi4jIporhiF9ERFKo8IuIFJmCKPxmdoqZTTOzKjMrqzbuajObaWYfmNm3a5i/l5lNNLMZZva4mTXOQsbHzWxy/DfbzCbXMN1sM3s3ni7rvz5jZtea2fyUbEfXMN1R8TacaWZXJZDrj2b2vpm9Y2ZPmVnrGqZLZHttbf3NrEn8Gs+M96We2cqSssxuZvaqmU2P9//L0kxzqJktT3l9f5XtXPFyt/i6WOQv8fZ6x8z6JpCpd8p2mGxmK8xsSLVpEtleZna/mS02s6kpw9qa2YtxHXrRzNrUMO/Z8TQzzOzsOgVw97z/A3YHegOjgLKU4XsAU4AmQC9gFlCSZv4ngNPi+3cDP8py3j8Bv6ph3GygXYLb7lrgiq1MUxJvux2BxvE23SPLuY4EGsb3bwRuDLW9Mll/4GLg7vj+acDjCbx2nYC+8f1WwIdpch0KjExqf8r0dQGOBv4LGNAPmJhwvhLgU6IvOCW+vYABQF9gasqwm4Cr4vtXpdvngbbAR/Ftm/h+m9ouvyCO+N19urt/kGbU8cBj7r7O3T8GZgL7p05gZgYMAobFgx4ETshW1nh5pwKPZmsZWbA/MNPdP3L39cBjRNs2a9z9BXeviB9OALpmc3lbkcn6H0+070C0Lx0Wv9ZZ4+4L3X1SfP9LYDrQJZvLrEfHA//wyASgtZl1SnD5hwGz3L2uPQJsE3cfAyytNjh1H6qpDn0beNHdl7r7F8CLwFG1XX5BFP4t6ALMTXk8j83/MXYAlqUUmXTT1Kf+wCJ3n1HDeAdeMLO3zOyCLOZIdUn8cfv+Gj5eZrIds+k8oqPDdJLYXpms/1fTxPvScqJ9KxFx09K+wMQ0ow80sylm9l8z2zOhSFt7XULvU6dR88FXiO0F0MHdF0L0pg60TzNNvWy3hnWKF4CZvQR0TDPqGnd/pqbZ0gyrfv1qJtNkJMOM32PLR/sHu/sCM2sPvGhm78dHB3W2pVzAXcD1ROt8PVEz1HnVnyLNvNt8HXAm28vMrgEqgEdqeJp6317poqYZlrX9qLbMrCXwJDDE3VdUGz2JqDljZXz+5mlglwRibe11Cbm9GgPHAVenGR1qe2WqXrZb3hR+dz+8DrPNA7qlPO4KLKg2zWdEHzMbxkdq6aapl4xm1hA4CfjmFp5jQXy72MyeImpm2KZClum2M7O/ASPTjMpkO9Z7rvjE1WDgMI8bONM8R71vrzQyWf+N08yLX+ft2fyjfL0zs0ZERf8Rdx9efXzqG4G7P2tmd5pZO3fPaodkGbwuWdmnMvQdYJK7L6o+ItT2ii0ys07uvjBu9lqcZpp5ROchNupKdG6zVgq9qWcEcFp8xUUvonfuN1IniAvKq8DJ8aCzgZo+QWyrw4H33f0sF/4AAAJCSURBVH1eupFm1sLMWm28T3SCc2q6aetLtXbVE2tY3pvALhZd/dSY6GPyiCznOgr4GXCcu6+uYZqktlcm6z+CaN+BaF96paY3q/oSn0O4D5ju7rfUME3HjecazGx/ov/5z7OcK5PXZQRwVnx1Tz9g+cZmjgTU+Kk7xPZKkboP1VSHngeONLM2cbPskfGw2sn22esk/ogK1jxgHbAIeD5l3DVEV2R8AHwnZfizQOf4/o5EbwgzgX8BTbKU8+/ARdWGdQaeTckxJf6bRtTkke1t9xDwLvBOvON1qp4rfnw00VUjsxLKNZOoLXNy/Hd39VxJbq906w/8huiNCaBpvO/MjPelHRPYRocQfcx/J2U7HQ1ctHE/Ay6Jt80UopPkByWQK+3rUi2XAXfE2/NdUq7Gy3K25kSFfPuUYYlvL6I3noXAhrh2nU90TuhlYEZ82zaetgy4N2Xe8+L9bCZwbl2Wry4bRESKTKE39YiISDUq/CIiRUaFX0SkyKjwi4gUGRV+EZEio8IvIlJkVPhFRIqMCr9IHZjZfnHHdk3jb6pOM7O9QucSyYS+wCVSR2b2W6Jv7DYD5rn7HwJHEsmICr9IHcX99rwJrCX6an9l4EgiGVFTj0jdtQVaEv36VdPAWUQypiN+kToysxFEv8bVi6hzu0sCRxLJSN70xy+SS8zsLKDC3f9pZiXAeDMb5O6vhM4msjU64hcRKTJq4xcRKTIq/CIiRUaFX0SkyKjwi4gUGRV+EZEio8IvIlJkVPhFRIrM/wN8zkgs1WZFkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of PReLU\n",
    "x = np.arange(-10, 10, .1)\n",
    "alpha = .1\n",
    "PReLU = np.maximum(x, alpha*x)\n",
    "\n",
    "plt.plot([-10, 10],[0, 0], c='k')\n",
    "plt.plot([0, 0],[-1, 10], c='k')\n",
    "plt.plot(x, PReLU, c='orange')\n",
    "plt.title(\"Leaky ReLU / PReLU\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Leaky ReLU / PReLU (Parametric -)\n",
    "$$\n",
    "    f(x) = max(0.01x, x)\\\\\n",
    "    f(x) = max(\\alpha x, x)\n",
    "$$\n",
    "* Solves the saturation problem, while keeping the pros of ReLU. \n",
    "    * This doesn't seem to be only good, since it loses some robustness to noise.\n",
    "\n",
    "* $\\alpha$ is **not** a hyperparameter, it's a parameter that should be updated with backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization of ELU\n",
    "x = np.arange(-5, 0, .1)\n",
    "alpha = .5\n",
    "ELU = alpha * (np.exp(x)-1)\n",
    "\n",
    "plt.plot([-5, 5],[0, 0], c='k')\n",
    "plt.plot([0, 0],[0, 5], c='k')\n",
    "plt.plot(x, ELU, c='orange')\n",
    "plt.plot([0, 5], [0, 5], c='orange')\n",
    "plt.title(\"ELU\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ELU (Exponential Linear Units)\n",
    "$$\n",
    "f(x) = \\left( \\begin{matrix} x & ,x>0 \\\\ \\alpha(exp(x)-1) & ,x\\leq 0 \\end{matrix} \\right.\n",
    "$$\n",
    "* All of ReLU + Closer zero-centering\n",
    "* Negative saturation region gives some robustness to noise\n",
    "* But... exp() is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "* What does saturation on negative regime have to do with robustness to noise?  \n",
    "I think 'killing' the gradients in this case can be helpful for 'ignoring' some noisy data. Some datas can form an island, which drops val accuracy when we try to overfit. On leaky ReLU all those data will have a gradient, constantly causing updates. On ELU or ReLU however, the classifier have a choice to put them on regative regime, thus ignoring them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Maxout \"Neuron\"\n",
    "$$\n",
    "f(x) = max(w^T_1x+b_1, w^T_2x+b_2)\n",
    "$$\n",
    "* Generalizes ReLU and Leaky ReLU\n",
    "* All parts are linear, doesn't die!\n",
    "* But... double parameters is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "* Is it possible that $w_1$ and $w_2$ will learn to be the exact same? It could be pretty fatal, because the activation function will basically be linear!  \n",
    "Well not quite fatal, since being linear means being nothing. It's rather a waste.\n",
    "\n",
    "\n",
    "* Can there be a way to use this technique, but with less parameters? Like making only half extra parameters to replace random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tl;dr\n",
    "\n",
    "Use : ReLU (But keep an eye on learning rates, and the number of dead ReLU's.)  \n",
    "Maybe : Leaky ReLU / Maxout / ELU  \n",
    "Eh... : tanh  \n",
    "No. : sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning is useless?\n",
    "\n",
    "* It is said that a 2 layer network with a sigmoidal function can approximate **any** continuous function. So why bother using 10 layer networks?\n",
    "\n",
    "\n",
    "* Well, it **can** do, but as much as the sloth in Zootopia can complete a full marathon. Deep learning can do infinitely better. Especially in convnet, since it carries the idea of hierachical observation (edges/corners -> face parts -> face)\n",
    "\n",
    "\n",
    "* [Getting a good intuition of this seems pretty cool](http://neuralnetworksanddeeplearning.com/chap4.html). Put simple, we can approximate an activation function to become a *step* function, such that\n",
    "$$\n",
    "f(x) = \\left( \\begin{matrix} 1 & ,x\\leq k \\\\ 0 & ,x>k \\end{matrix} \\right.\n",
    "$$\n",
    "and we'll be able to approximate any function that satisfies an arbitrary error rate $\\epsilon > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size\n",
    "\n",
    "* Bigger layer = More hidden neurons = More complex functions = More *capacity*\n",
    "* Which leads to a more accurate, yet overfitting model.\n",
    "* We need to keep it simple and thus more *general*, so should we use small layer size? NO!\n",
    "* Smaller networks usually have little local minima, but most of them turn out to be terrible(high loss). Bigger networks, on the other hand, has much more local minimas. They however are usually have much better loss.\n",
    "* We can get generality by other measures(e.g. Dropout, regularization, input noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "* I absolutely have no idea what in heavens is going on [here](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Both mean subtraction and normalization makes the loss function less sensitive to the weights. The intuition is that as we move further away from the origin, \n",
    "\n",
    "#### 1. Mean subtraction\n",
    "* Subtracting mean image (Pixel-wise mean)\n",
    "* Subtracting mean (Total mean)\n",
    "* Subtracting mean channel (Channel-wise mean)\n",
    "    \n",
    "    \n",
    "#### 2. Normalization\n",
    "* Dividing by total std\n",
    "* Dividing by pixel-wise std\n",
    "    * Use when it's sure that each dimension has different distribution, yet they have same importance to making a prediction. \n",
    "\n",
    "* In case of images, each dimension(pixel) has similar range(0-255), so it's not quite necessary to do this.\n",
    "\n",
    "#### 3. PCA / Whitening\n",
    "* TODO fill this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "#### 1. All zero\n",
    "* With zero centered/normalized data, the trained final weights are probably half positive and half negative.\n",
    "* So the intuition here is that we let the classifier choose which ones to put positive or negative.\n",
    "* Problem :  Since all scores will be the same, all gradients will be the same, and all templates will be updated in the exact same direction. As a result, it'll be no better than having just one template.\n",
    "\n",
    "#### 2. Random numbers\n",
    "* Uniform initialization is problematic. OK, but we still want them to be near 0. Then random numbers near 0 should do the trick!\n",
    "* Variance\n",
    "    * When the random weights are too small, the activation(output matrix) from each layer will become near uniform 0. The same thing will happen in backprop, so the gradients will also go to zero.\n",
    "    * When random weights are too big, the activation will also become too big and sparse, which usually kills the gradient.\n",
    "* Problem : The variance of activation is proportional to the number of inputs, since more inputs mean more values to sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h =  3\n",
      "iter 0 : 0.05037867860026177\n",
      "iter 1 : 0.05702380733721083\n",
      "iter 2 : 1.0080950025962847\n",
      "iter 3 : 11.211647459649079\n",
      "iter 4 : 9.863022188477165\n",
      "iter 5 : 81.66260385676888\n",
      "iter 6 : 15.554132224388951\n",
      "iter 7 : 173.38432641188604\n",
      "iter 8 : 222.92927717257717\n",
      "iter 9 : 1255.8858946360613\n",
      "iter 10 : 2928.165968408363\n",
      "\n",
      "h =  10\n",
      "iter 0 : 0.7965001921751311\n",
      "iter 1 : 4.492322272212002\n",
      "iter 2 : 96.94766997303098\n",
      "iter 3 : 473.0146128091228\n",
      "iter 4 : 6895.35595600622\n",
      "iter 5 : 99325.3831602217\n",
      "iter 6 : 856412.6958530731\n",
      "iter 7 : 6054665.269803217\n",
      "iter 8 : 62213701.96443077\n",
      "iter 9 : 381088106.1562104\n",
      "iter 10 : 2327421911.2049294\n",
      "\n",
      "h =  30\n",
      "iter 0 : 0.6935029945895187\n",
      "iter 1 : 18.088699863554627\n",
      "iter 2 : 614.8829578072001\n",
      "iter 3 : 19481.97585974153\n",
      "iter 4 : 515853.7350596593\n",
      "iter 5 : 13392298.496776136\n",
      "iter 6 : 512723257.7621299\n",
      "iter 7 : 16242111660.168684\n",
      "iter 8 : 352747659393.64465\n",
      "iter 9 : 8527591249217.054\n",
      "iter 10 : 243982283234022.38\n"
     ]
    }
   ],
   "source": [
    "# Activation variance\n",
    "# Ignore the exploding values,\n",
    "# focus on how fast the variance grow with each hidden size.\n",
    "hidden_sizes = [3, 10, 30]\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    x = np.random.randn(h)\n",
    "    print(\"\\nh = \", h)\n",
    "    print(\"iter 0 :\", np.var(x))\n",
    "    for i in range(10):\n",
    "        hidden = np.random.randn(h, h)       \n",
    "        x = np.dot(x, hidden)\n",
    "        print(\"iter\",i+1,\":\", np.var(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Xavier\n",
    "* We want to keep the output variance as the input variance. Such that $Var(s) = Var(x)$\n",
    "\n",
    "* Match the variance of W to $1/n$ by multiplying $1/\\sqrt{n}$, where n is the number of inputs\n",
    "    * In case of conv layers, n is still the total number of parameters. (i.e. filter_size^2 * number_of_filters)\n",
    "    \n",
    "\n",
    "* Proof\n",
    "$$\n",
    "Var(s) = Var\\left( \\sum^n_i w_ix_i \\right)\n",
    "$$\n",
    "  Where $s$ and all of $w_i$,$x_i$ are all random variables.  \n",
    "  And because all $w_i$'s and $x_i$'s are independent,\n",
    "$$\n",
    "= \\sum^n_i Var(w_ix_i)\n",
    "$$\n",
    "  (proof below)\n",
    "$$\n",
    "= \\sum^n_i [E(w_i)]^2Var(x_i) + [E(x_i)]^2Var(w_i) + Var(w_i)Var(x_i) \\\\\n",
    "$$\n",
    "  Since we zero-center both inputs and weights, $E(x_i) = E(w_i) = 0$.  \n",
    "  So\n",
    "$$\n",
    "= \\sum^n_i Var(w_i)Var(x_i)\n",
    "$$\n",
    "  and as a result,\n",
    "$$\n",
    "Var(s) = (nVar(w))Var(x)\n",
    "$$\n",
    "\n",
    "\n",
    "* So for $Var(s) = Var(x)$,\n",
    "$$\n",
    "W := \\frac{W}{\\sqrt{n}}\n",
    "$$  \n",
    "such that $$\n",
    "Var\\left(\\frac{W}{\\sqrt{n}}\\right) =  Var(W)\\frac{1}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Xavier nooo\n",
    "\n",
    "* Product distribution\n",
    "    * Assume two random variables $X, Y$ and their p.d.f $f_X(x), f_Y(y)$\n",
    "    * Assume $Z = XY$. How can $f_Z(z)$ be expressed?  \n",
    "    Use that $ \\frac{dF_Z(z)}{dz} = f_Z(z)$\n",
    "    $$\n",
    "    \\int_{-\\infty}^z f_Z(\\tau) d\\tau = F_Z(z) = P(XY\\leq z)\n",
    "    $$\n",
    "    Divide by X's sign\n",
    "    $$\n",
    "    = P(XY \\leq z, X \\geq 0) + P(XY \\leq z, X \\leq 0)\n",
    "    $$\n",
    "    such that\n",
    "    $$\n",
    "    = P(Y \\leq \\frac{z}{X}, X \\geq 0) + P(Y \\geq \\frac{z}{X}, X \\leq 0)\n",
    "    $$\n",
    "    express in integral\n",
    "    $$\n",
    "    = \\int^{\\infty}_0 \\int^{\\frac{z}{x}}_{-\\infty} f_X(x)f_Y(y) dydx + \\int^0_{-\\infty} \\int^{\\infty}_{\\frac{z}{x}} f_X(x)f_Y(y) dydx  \n",
    "    $$\n",
    "    or\n",
    "    $$\n",
    "    = \\int^{\\infty}_0 f_X(x) \\int^{\\frac{z}{x}}_{-\\infty} f_Y(y) dydx + \\int^0_{-\\infty} f_X(x) \\int^{\\infty}_{\\frac{z}{x}} f_Y(y) dydx  \n",
    "    $$\n",
    "    Now that we have $f_Z(z)$, take the derivative from both sides.  \n",
    "    And by chain rule,\n",
    "    $$\n",
    "    f(z) = \\int^{\\infty}_0 f_X(x)f_Y\\left(\\frac{z}{x}\\right)\\frac{1}{x} dx - \\int^0_{-\\infty} f_X(x)f_Y\\left(\\frac{z}{x}\\right)\\frac{1}{x} dx\n",
    "    $$\n",
    "    as a result,\n",
    "    $$\n",
    "    f(z) = \\int^{\\infty}_{-\\infty} f_X(x)f_Y\\left(\\frac{z}{x}\\right)\\frac{1}{|x|} dx \n",
    "    $$\n",
    "\n",
    "\n",
    "* Product mean\n",
    "    * How can $E(Z)$ be expressed?  \n",
    "    By the definition of covariance,\n",
    "    $$\n",
    "    cov(X,Y) = E(XY) - E(X)E(Y)\n",
    "    $$\n",
    "    so\n",
    "    $$\n",
    "    E(Z) = cov(X,Y) + E(X)E(Y)\n",
    "    $$\n",
    "\n",
    "* Product variance\n",
    "    * How can $Var(Z)$ be expressed?  \n",
    "    Use the product mean $E(Z) = cov(X,Y) + E(X)E(Y)$ <br> <br>\n",
    "    $$\n",
    "    \\begin{matrix}\n",
    "    Var(XY) & = & E(X^2Y^2) - [E(XY)]^2 \\\\ \\\\\n",
    "     & = & cov(X^2, Y^2) + E(X^2)E(Y^2) - [E(XY)]^2 \\\\ \\\\\n",
    "     & = & cov(X^2, Y^2) + (Var(X) + [E(X)]^2)(Var(Y) + [E(Y)]^2) - (cov(X,Y)+E(X)E(Y))^2 \\\\ \\\\ \n",
    "    \\end{matrix}\n",
    "    $$\n",
    "    **If X and Y are independent** (e.g. RGB values and weights in neural network),  \n",
    "    the variables are uncorrelated, so all correlation coefficients are 0.\n",
    "    $$\n",
    "    = (Var(X) + [E(X)]^2)(Var(Y) + [E(Y)]^2) - [E(X)E(Y)]^2\n",
    "    $$\n",
    "    as a result,\n",
    "    $$\n",
    "    Var(XY) = [E(X)]^2Var(Y) + [E(Y)]^2Var(X) + Var(X)Var(Y)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h =  3 , Xavier: True\n",
      "iter 0 : 0.2571882529542194\n",
      "iter 1 : 0.22610851761038409\n",
      "iter 2 : 0.05388918769231843\n",
      "iter 3 : 0.023192161952574716\n",
      "iter 4 : 0.0004110977211831653\n",
      "iter 5 : 0.0031350507615370477\n",
      "iter 6 : 0.0002303150221736258\n",
      "iter 7 : 0.007360682930955668\n",
      "iter 8 : 0.002883339384947295\n",
      "iter 9 : 0.006381112568295329\n",
      "iter 10 : 0.008000758146960225\n",
      "\n",
      "h =  10 , Xavier: True\n",
      "iter 0 : 0.4870417457500216\n",
      "iter 1 : 1.043353525785387\n",
      "iter 2 : 0.7088567007187313\n",
      "iter 3 : 0.6917093722616752\n",
      "iter 4 : 0.7472061238722276\n",
      "iter 5 : 0.7613808074171756\n",
      "iter 6 : 0.62472156560976\n",
      "iter 7 : 0.5122619506966843\n",
      "iter 8 : 1.3765585221349421\n",
      "iter 9 : 1.197484431046083\n",
      "iter 10 : 1.8070629500189153\n",
      "\n",
      "h =  30 , Xavier: True\n",
      "iter 0 : 1.2062658668400912\n",
      "iter 1 : 1.7315794292099718\n",
      "iter 2 : 1.0294142966143172\n",
      "iter 3 : 2.165472684546356\n",
      "iter 4 : 1.676739570259764\n",
      "iter 5 : 1.5307424519186499\n",
      "iter 6 : 1.5480886096773465\n",
      "iter 7 : 1.395255310128029\n",
      "iter 8 : 1.243177885044761\n",
      "iter 9 : 1.3486102643523539\n",
      "iter 10 : 1.7992142755696223\n",
      "\n",
      "h =  1500 , Xavier: True\n",
      "iter 0 : 1.059215116928066\n",
      "iter 1 : 1.0475542359734484\n",
      "iter 2 : 1.0484106185547186\n",
      "iter 3 : 1.0194487200230393\n",
      "iter 4 : 1.0317457994630985\n",
      "iter 5 : 1.1376336335336485\n",
      "iter 6 : 1.0752017477074567\n",
      "iter 7 : 1.0408505582401089\n",
      "iter 8 : 1.013625190060961\n",
      "iter 9 : 1.013140131877602\n",
      "iter 10 : 1.0164613701586636\n"
     ]
    }
   ],
   "source": [
    "# Activtion variance with xavier\n",
    "# Seems to work better as the size increases\n",
    "hidden_sizes = [3, 10, 30, 1500]\n",
    "xavier = True\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    x = np.random.randn(h)\n",
    "    print(\"\\nh = \", h, \", Xavier:\", xavier)\n",
    "    print(\"iter 0 :\", np.var(x))\n",
    "    for i in range(10):\n",
    "        hidden = np.random.randn(h, h)\n",
    "        if (xavier):\n",
    "            hidden /= np.sqrt(h)\n",
    "       \n",
    "        x = np.dot(x, hidden)\n",
    "        print(\"iter\",i+1,\":\", np.var(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. More than Xavier\n",
    "* He initialization\n",
    "    * After ReLU, half of activation will become 0. This will fold the variance by half everytime.\n",
    "    * So what we actually want for the output variance is not to be the same as input variance, but **twice** as much.\n",
    "    $$\n",
    "    W:= W\\sqrt{\\frac{2}{n}}\n",
    "    $$\n",
    "\n",
    "* Some suggestion\n",
    "    * Try considering output size along with input size. $Var(w) = 2/(n_{in} + n_{out})$\n",
    "    \n",
    "#### 5. Bias initialization\n",
    "* Since initial values are extremely small, it is possible that the ReLU could not give proper gradients.\n",
    "* So to solve that, some people like to initialize biases with small constant value (e.g. 0.01)\n",
    "* However it's not sure that this works as intended. Some cases even come out to be otherwise.\n",
    "* It's more common to use 0 initialization for biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h =  10 , ReLU: True , Xavier: True , sqrt: True\n",
      "iter 0 : 1.423037082289094\n",
      "iter 1 : 0.1318823115934617\n",
      "iter 2 : 0.050166295983738673\n",
      "iter 3 : 0.09859039490900613\n",
      "iter 4 : 0.14426800433467207\n",
      "iter 5 : 0.5002692029704352\n",
      "iter 6 : 0.24244127283635838\n",
      "iter 7 : 0.1551592962595018\n",
      "iter 8 : 0.09751763937510743\n",
      "iter 9 : 0.14228968016705373\n",
      "iter 10 : 0.2528308747985054\n",
      "\n",
      "h =  100 , ReLU: True , Xavier: True , sqrt: True\n",
      "iter 0 : 1.2022664904623452\n",
      "iter 1 : 0.7992476423347596\n",
      "iter 2 : 0.6188430033799187\n",
      "iter 3 : 0.5390716600200968\n",
      "iter 4 : 0.4029714081216565\n",
      "iter 5 : 0.3556424235579992\n",
      "iter 6 : 0.38875170955820076\n",
      "iter 7 : 0.5843759475556153\n",
      "iter 8 : 0.6752559974009206\n",
      "iter 9 : 0.6502836809413306\n",
      "iter 10 : 0.7187391020599107\n",
      "\n",
      "h =  1500 , ReLU: True , Xavier: True , sqrt: True\n",
      "iter 0 : 0.9505930364091709\n",
      "iter 1 : 0.6680577885316701\n",
      "iter 2 : 0.667692777696497\n",
      "iter 3 : 0.6129175287632036\n",
      "iter 4 : 0.6166500345462389\n",
      "iter 5 : 0.6002702627742302\n",
      "iter 6 : 0.5992601462680731\n",
      "iter 7 : 0.6027738283642772\n",
      "iter 8 : 0.6204303756631806\n",
      "iter 9 : 0.5842789063359379\n",
      "iter 10 : 0.5808799614378573\n"
     ]
    }
   ],
   "source": [
    "# Kaiming / MSRA initalization\n",
    "# Seems to work better as the size increases\n",
    "hidden_sizes = [10, 100, 1500]\n",
    "\n",
    "xavier = True # Performs xavier initialization\n",
    "sqrt = True # Performs additional sqrt(2) initalization\n",
    "ReLU = True # Adds ReLU layer\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    x = np.random.randn(h)\n",
    "    print(\"\\nh = \", h, \", ReLU:\", ReLU, \", Xavier:\", xavier, \", sqrt:\", sqrt)\n",
    "    print(\"iter 0 :\", np.var(x))\n",
    "    for i in range(10):\n",
    "        hidden = np.random.randn(h, h)\n",
    "        if (xavier):\n",
    "            hidden /= np.sqrt(h)\n",
    "            if (sqrt):\n",
    "                hidden *= np.sqrt(2)\n",
    "       \n",
    "        x = np.dot(x, hidden)\n",
    "        if (ReLU):\n",
    "            x[x<0] = 0\n",
    "        print(\"iter\",i+1,\":\", np.var(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Batch normalization\n",
    "* You want unit gaussian every time? Make them so!\n",
    "* Also add another flexibility by giving an additional mean and std parameter.\n",
    "    * After training, those will basically become an identity function ($\\gamma = 1$ and $\\beta = 0$).\n",
    "* Normalize and re-shift each $j$th dimension separately. ($\\epsilon$ is for numerical stability)\n",
    "    * For conv layers, normalize each $j$th filter.\n",
    "$$\n",
    "x_{i,j} = \\gamma_j \\frac{x_{i,j} - \\mu_j}{\\sqrt{\\sigma^2_j+\\epsilon}} + \\beta_j\n",
    "$$\n",
    "\n",
    "* But mean and std cannot be computed at test time, because it's just one image!\n",
    "    * Average out the history of mean and std we got mid-training, and use that for test time.\n",
    "    * This could be problematic, since it works differently training and testing.\n",
    "    \n",
    "    \n",
    "    \n",
    "* Easier training, better gradient, faster convergence.\n",
    "* Robustness to initialization, works as regularization.\n",
    "\n",
    "#### 7. More than batch normalization\n",
    "* Layer normalization\n",
    "    * Instead of dimension-wise/filter-wise normalization, normalize for every image.\n",
    "\n",
    "* Instance normalization (conv)\n",
    "    * Instead of filter-wise normalization, normalize for every pixel of each filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tl;dr\n",
    "* Activation function : Use ReLU\n",
    "* Data preprocessing : Subtract mean image or mean RGB\n",
    "* Weight initialization : Use Xavier or He\n",
    "* Batch Normalization : Good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "* Searching\n",
    "    * Search in log scale\n",
    "    * Do a coarse search first, then go specific.\n",
    "    * If good results are at the edge, maybe the range is not so good.\n",
    "    * Instead of grid search, do **random search**. In grid search you're checking the same parameter multiple times, which is a waste.\n",
    "  \n",
    "  \n",
    "* Learning rate\n",
    "![lr](images\\learningrates.jpeg)\n",
    "\n",
    "\n",
    "* Initialization\n",
    "    * If the loss doesn't go down for a while and suddenly starts to drop, suspect bad initialization.\n",
    "    \n",
    "\n",
    "* Train acc vs Val acc\n",
    "    * The gap between two acc's means the degree of overfitting.\n",
    "    * If the gap is too big, it needs more regularization.\n",
    "    * If there's no gap, maybe the model can do better (e.g. low capacity(hidden layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "#### 1. Vanilla gradient descent (SGD)\n",
    "* Problem\n",
    "    * When the landscape is ill-conditioned(e.g. oval contours), the \n",
    "    * On local minima or saddle point, the gradient is zero, and SGD gets stuck.\n",
    "        * In practice, saddle points are much more common than local minimas.\n",
    "        * Local minimas are rare, since it means all possible directions makes the loss grow.\n",
    "    * 'Stochastic' makes noisy movements.\n",
    "    \n",
    "#### 2. SGD + Momentum\n",
    "* The gradients are accumulated with certain friction.\n",
    "$$\n",
    "\\begin{matrix}\n",
    "v_{t+1} & = & \\rho v_{t} + \\nabla f(x_t) \\\\\n",
    "x_{t+1} & = & x_t - \\alpha v_{t+1}\n",
    "\\end{matrix}\n",
    "$$\n",
    "* Where $\\rho$ is the friction. Usually $\\rho$= .9 or .99\n",
    "\n",
    "* Can solve ill-conditioning, as sensitive dimensions will constantly get opposite directions that kill each other. Other dimensions will gain momentum in the same direction, and thus will converge faster.\n",
    "* Can solve local minima and saddle points, since the update doesn't rely solely on immedate gradients.\n",
    "\n",
    "#### 3. SGD + Nesterov Momentum\n",
    "* This time, the gradient ($\\nabla f(x_t)$) is not computed on current point. Rather, it's evaluated at the point where the *current velocity wants to go*. This works as if the gradient is checking if the velocity is going well, and gives it a chance to *fix* it to the correct direction.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "v_{t+1} & = & \\rho v_t - \\alpha \\nabla f(x_t + \\rho v_t) \\\\\n",
    "x_{t+1} & = & x_t + v_{t+1}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\alpha$ works just as the same next to $\\nabla f$, since all gradients are multiplied by $\\alpha$ only once. And since each gradient is being *subtracted* to the velocity, the velocity itself must be added to the position.\n",
    "\n",
    "\n",
    "\n",
    "* But this equation is not quite good in practice. We need the gradients in $\\nabla f(x_t)$ for backpropagation, but the expression above has that extra $\\rho v_t$.  \n",
    "  Let's fix that!\n",
    "\n",
    "  Let\n",
    "$$\n",
    "\\tilde{x}_t = x_t + \\rho v_t \\\\\n",
    "\\tilde{x}_{t+1} = x_{t+1} + \\rho v_{t+1}\n",
    "$$\n",
    "  So the first equation becomes\n",
    "$$\n",
    "v_{t+1} = \\rho v_t - \\alpha \\nabla f(\\tilde{x_t})\n",
    "$$\n",
    "  and the second one becomes\n",
    "$$\n",
    "\\tilde{x}_{t+1} - \\rho v_{t+1} = \\tilde{x}_t - \\rho v_t + v_{t+1} \\\\\n",
    "\\tilde{x}_{t+1} = \\tilde{x}_t +  \\rho (v_{t+1} - v_t) + v_{t+1}\n",
    "$$\n",
    "\n",
    "  As a result,\n",
    "  $$\n",
    "  \\begin{matrix}\n",
    "  v_{t+1} & = & \\rho v_t - \\alpha \\nabla f(x_t) \\\\\n",
    "  x_{t+1} & = & x_t +  \\rho (v_{t+1} - v_t) + v_{t+1}\n",
    "  \\end{matrix}\n",
    "  $$\n",
    "  \n",
    "  Instead of following the current position, it's keeping track of *\"the point where velocity wants to go\"*. And that's easier because that's the part where we evaluate our gradient.\n",
    "  \n",
    "  \n",
    "* Momentum passing by sharp minima's\n",
    "    * Sharp minima is not a good place to land at the first place. When the dataset changes, the whole landscpae will change. And sharp minimas are prone to disappear, making the local point useless. What we want is more flat and stable minima, and momentum can help this by passing through the sharp minima's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. AdaGrad\n",
    "* Instead of momentum, keep the running squared sum of gradients overtime.  \n",
    "When updating, divide the gradient by the square root of the sum.\n",
    "$$\n",
    "SSG := SSG + (\\nabla f(x))^2 \\\\\n",
    "x := x - \\alpha \\nabla f(x) \\frac{1}{\\sqrt{SSG} + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "* In an ill-conditioned convex, sensitive dimensions will have high SSG values and low SSG for otherwise. High SSG means smaller step size and decceleration, while small SSG means bigger step size and acceleration. As a result, AdaGrad tries to make equal relative steps for each dimensions.\n",
    "\n",
    "* However, as SSG accumulates, all step sizes will eventually become very small. This can be effective for convex problems, but this is a problem on non-convex problems such as saddle points.\n",
    "\n",
    "#### 5. RMSProp\n",
    "* Addresses AdaGrad's problem by implementing a decay rate to SSG.\n",
    "$$\n",
    "SSG := \\gamma SSG + (1 - \\gamma) (\\nabla f(x))^2 \\\\\n",
    "x -= x - \\alpha \\nabla f(x) \\frac{1}{\\sqrt{SSG} + \\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "* It can be interpreted as mixing current SSG and new gradient by the ratio of $SSG : \\nabla f(x) = 1-\\gamma : \\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Adam\n",
    "* Momentum and RMSProp are both good. Why not both?\n",
    "$$\n",
    "v := \\beta_1 v +  (1 - \\beta_1)\\nabla f(x) \\\\\n",
    "SSG := \\beta_2 SSG + (1-\\beta_2)(\\nabla f(x))^2 \\\\\n",
    "x := x - \\alpha \\frac{v}{\\sqrt{SSG} + \\epsilon}\n",
    "$$\n",
    "\n",
    "* Seems nice, but this is problematic at the first step.\n",
    "    * The first moment(v) and the second moment(SSG) after first step will be near 0, and when $\\beta_2 \\simeq 0.9$ the stepsize can become extremely big.\n",
    "    * To fix this, Adam adds another **bias correction term**. This prevents those values from being too small in the beginning.\n",
    "    * For $t$th iteration,\n",
    "    \n",
    "$$\n",
    "v := \\frac{\\beta_1 v +  (1 - \\beta_1)\\nabla f(x)}{1 - \\beta_1^t} \\\\\n",
    "SSG := \\frac{\\beta_2 SSG + (1-\\beta_2)(\\nabla f(x))^2}{1 - \\beta_2^t} \\\\\n",
    "x := x - \\alpha \\frac{v}{\\sqrt{SSG} + \\epsilon}\n",
    "$$\n",
    "\n",
    "  \n",
    "* A great starting optimization algorithm for many problems.\n",
    "    * Try $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, learning rate = 1e-3 or 5e-4\n",
    "    \n",
    "\n",
    "* One limitation\n",
    "    * Momentum, RMSProp, and Adam are still controlling accelerations among independent axes. This means if the convex is tilted against the axes diagonally, those won't be very effective as it is on axes-aligned convexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "\n",
    "Which learning rate should we choose? Do we need to use only ONE learning rate?\n",
    "\n",
    "* Usually works with SGD momentum, and not quite with Adam.\n",
    "* If done well, you can see that the loss suddenly goes down as soon as lr drops.\n",
    "\n",
    "#### 1. Step decay\n",
    "* Decay every certain steps. (e.g. ResNet  $\\alpha := 0.1\\alpha$ every 30 epochs)\n",
    "\n",
    "#### 2. Cosine decay\n",
    "* Decay with cosine curve\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2}\\alpha_0(1+cos\\left(\\frac{t\\pi}{T}\\right))\n",
    "$$\n",
    "\n",
    "Where $\\alpha_t$ is a learning rate for $t$th epoch and $T$ is the number of total epochs \n",
    "\n",
    "#### 3. Linear decay\n",
    "$$\n",
    "\\alpha_t = \\alpha_0(1-\\frac{t}{T})\n",
    "$$\n",
    "\n",
    "#### 4. Inverse square root\n",
    "$$\n",
    "\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t}} \n",
    "$$\n",
    "\n",
    "#### 5. Exponential decay\n",
    "$$\n",
    "\\alpha_t = \\alpha_0 e^{-t/T}\n",
    "$$\n",
    "\n",
    "#### 6. Linear warmup\n",
    "* Giving high learing rate right over can make the loss explode.\n",
    "* Start from 0, and linearly increase it for the first ~5000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS\n",
    "\n",
    "* Second-order optimization\n",
    "    * What we've been doing was first-order optimization: get the gradient and use linear approximation. Since it's drawing a tangent line to descent, we can't go very far without making a huge error.\n",
    "    * What we can do is use **Hessian** in addition to gradients to form a **quadratic approximation**. We can optimize by moving to the minima in the quadratic.\n",
    "    * And since we have an exact point to move, we don't need a learning rate(at least not in vanilla version)!\n",
    "    \n",
    "    $$\n",
    "    \n",
    "    $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
